# -*- coding: utf-8 -*-
# @Time    : 2020/1/3 20:59
# @Author  : MMY
# @Email   : mmy_fly5381@163.com
# @File    : Word_prediction.py
# @Software: Pytharm
import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt

torch.manual_seed(0)
#  training sentences and their corresponding word-tags
training_data = [
    ('The cat ate the cheese'.lower().split(), ['DET', "NN", 'V', 'DET', 'NN']),
    ('She reads that book , fish , and cartoon'.lower().split(), ['NN', 'V', 'DET', 'NN','SYM','NN', 'SYM', 'DET', 'NN']),
    ('The dog loves art , while panda likes bamboo'.lower().split(), ['DET', 'NN', 'V', 'NN','SYM', 'CONJ','NN', 'V', 'NN']),
    ('The elephant answers the phone'.lower().split(), ['DET', 'NN', 'V', 'DET', 'NN'])
]
print('training_data: \n', training_data)



# create a dictionary that maps words to indices
word2idx = {}
for sent, tags in training_data:
    # print('sent:', sent)
    # print('tags:', tags)
    for word in sent:
        # print('word:', word)
        if word not in word2idx:
            word2idx[word] = len(word2idx)
    # print('word2idx:', word2idx)
    pass
print('word2idx: \n', word2idx)
# create a dictionary that maps tags to indices
tag2idx = {"DET": 0, "NN": 1, "V": 2, 'SYM': 3, 'CONJ': 4}
#  prepare the testing data which never shown in the training set
testing_data = 'She loves dog , elephant while the panda likes cartoon '    # NN, V, NN, SYM, NN, CONJ, DET, NN, V, NN
correct_id = np.array([1, 2, 1, 3, 1, 4, 0, 1, 2, 1])
# a helper function for converting a sequence of words to a Tensor of numerical values
# will be used later in training
def prepare_sequence(seq, to_idx):
    '''This function takes in a sequence of words and returns a
    corresponding Tensor of numerical values (indices for each word).'''
    idxs = [to_idx[w] for w in seq]
    idxs = np.array(idxs)
    return torch.from_numpy(idxs).type(torch.LongTensor)


class LSTMTagger(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):
        ''' Initialize the layers of this model.'''
        super(LSTMTagger, self).__init__()

        self.hidden_dim = hidden_dim

        # embedding layer that turns words into a vector of a specified size
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        # the LSTM takes embedded word vectors (of a specified size) as inputs
        # and outputs hidden states of size hidden_dim
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

        # the linear layer that maps the hidden state output dimension
        # to the number of tags we want as output, tagset_size (in this case this is 3 tags)
        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)


    def forward(self, sentence):
        ''' Define the feedforward behavior of the model.'''
        # create embedded word vectors for each word in a sentence
        embeds = self.word_embeddings(sentence)
        # print('embeds:', embeds)
        # get the output and hidden state by passing the lstm over our word embeddings
        # the lstm takes in our embeddings and  hiddent state
        training_data = embeds.view(len(sentence), 1, -1)  # lstm default input (time_step, batch_size, inpu_size)
        # print('training data:', training_data)
        lstm_out, self.hidden = self.lstm(training_data, None)  # lstm_out (time_step, batch, output_size)
        # print('lstm out:', lstm_out)
        # print('reshape lstm out:', lstm_out.view(len(sentence), -1))
        # get the scores for the most likely tag for a word
        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))
        tag_scores = F.log_softmax(tag_outputs, dim=1)

        return tag_scores


# the embedding dimension defines the size of our word vectors
# for our simple vocabulary and training set, we will keep these small
EMBEDDING_DIM = 12
HIDDEN_DIM = 6
VOCAB_SIZE = len(word2idx)
TAGSET_SIZE = len(tag2idx)
EPOCH_NUM = 300
test_sample_id =2
# instantiate our model
model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE)

# define our loss and optimizer
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

test_sentence, target_tags = training_data[test_sample_id]
print('test sentence:', test_sentence)
test_targets = prepare_sequence(target_tags, tag2idx)
print('target id:', test_targets)
inputs = prepare_sequence(test_sentence, word2idx)
out = model(inputs)
predicted_testing_tags = torch.max(out, 1)[1]
print('Predicted id: ', predicted_testing_tags)

print('start training...')
for epoch in range(EPOCH_NUM):
    epoch_loss = 0.0
    # get all sentences and corresponding tags in the training data
    for sentence, tags in training_data:
        # zero the gradients
        # optimizer.zero_grad()
        model.zero_grad()
        # prepare the inputs for processing by out network,
        # turn all sentences and targets into Tensors of numerical indices
        sentence_in = prepare_sequence(sentence, word2idx)
        # print('sentence id:', sentence_in)
        targets = prepare_sequence(tags, tag2idx)

        tag_scores = model(sentence_in)
        loss = loss_function(tag_scores, targets)
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()

    # print out avg loss per 20 epochs
    if (epoch % 20 == 19):
        new_sentence = testing_data.lower().split()
        inputs = prepare_sequence(new_sentence, word2idx)
        out = model(inputs)
        predicted_testing_tags = torch.max(out, 1)[1].data.numpy()
        accuracy = float((predicted_testing_tags == correct_id).astype(int).sum()) / float(correct_id.size)
        print("Epoch: %d, loss: %1.5f" % (epoch + 1, epoch_loss / len(training_data)), '| test accuracy: %.2f' % accuracy)


test_sentence, target_tags = training_data[test_sample_id]
print('test sentence:', test_sentence)
test_targets = prepare_sequence(target_tags, tag2idx)
print('target id:', test_targets)

# see what the scores are after training
inputs = prepare_sequence(test_sentence, word2idx)
out = model(inputs)
predicted_testing_tags = torch.max(out, 1)[1]
print('Predicted id: ', predicted_testing_tags)

new_sentence = testing_data.lower().split()
print('new sentence:', new_sentence)
inputs = prepare_sequence(new_sentence, word2idx)
out = model(inputs)
predicted_testing_tags = torch.max(out, 1)[1]
print('correct id:', correct_id)
print('Predicted id: ', predicted_testing_tags)
